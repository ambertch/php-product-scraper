
Page retrieval and parsing: 
- I used curl since it has universal support across all web hosting providers. I did a simple implementation w/ only faking a user agent, ignoring faking login/cookies.  

- php 5's DOM extension allows us to parse the page. The string returned by curl is turned into a DOMDocument. Via the defaultScraper or site specific Handlers, that and a DOMXPath object along with an xpath query is passed to the universal scraper functions.
- I'm choosing to grab data with XPath instead of a simple regex/strpos(). I think there is an advantage in robustness for a production implementation, by traversing the DOM.

- having the universal scrapers return price and description as a DOMNodeList confers numerous advantages. For example it allows a custom Handler to process multiple pricings if the item is on sale, or if multiple pricing options are available. Similarly, a product's description may often be contained in multiple nodes - returning a single string is less conducive to semantic processing.

- site specific Handlers are kept in a subfolder and autoloaded as necessary. Each Handler is a subclass of ProductScraper. The Handler then calls its parent's universal scraper functions using its own query, and processes the result.  

- The default handler actually does not use the universal price scraper. This design decision was made because the default scraper should be as dumb and general as possible, so it will just preg_match() for a $ followed by a float. The custom handlers will use the universal scraper and do this in a more robust fashion, retrieving the known node containing the price.    



What this code is missing:
1. Error handling: I'd put some try/catch blocks for the numerous exceptions that could arise (for example in the autoloading process) 
2. The image scraper judges width based on the <img> attribute, not the true width from a downloaded image.
3. I suspect that PHP5's DOMNode->getAttribute() has a bug. Though I didn't want to get into a rathole with hypotheses as to why.



Bonus #1 
This doesn't follow your stipulation of "Adding/removing handlers should accessible through static methods", but I thought it was cool and nifty so I decided to see if it works:  Converting the hostname component of parse_url() to a legal variable name by replacing "." with "_" (www.amazon.com becomes www_amazon_com), we leverage php's class_exists() and autoload functionality. The script takes the converted url and first looks for a class of that name, then strips a subdomain and looks again. So if class books_amazon_com doesn't exist, it will look for class amazon_com. In this way wildcard handlers for subdomains (currently just for one level down) are implemented - if the subdomain handler doesn't exist, the script will just use the domain handler.


Bonus #2 
I did not implement anything from bonus #2, though here are some of the things I would do to address high volume:

- RE: " if we make a lot of requests for a particular product in a day, there's not much likelihood that the product information would change in a day" - if there is an intent to 'cache' scraping for recently requested product pages, we can check when a product was last requested. Presumably we are storing this information in a database, so we can have a 'last updated' column. Because the normalized url represent a unique identifier of a product, before doing the actual scraping the Handler will take the normalized url and check it against the database. If an existing entry is older than a threshold, a new scraping is done. Else, save some processing power. Included in a text config file would be a field to set that recency threshold (by default something like 7 days?) In addition, we may want to scrape everything periodically such that the db doesn't become stale, since I'm sure those product pages will go away once an item goes out of season/is no longer carried.

- If the tables of products grow huge, in addition we may chose to optimize the schema. For example the list of images on each product page is an array of variable length. Database rules for first normal form dictate I should store these in a separate "sites-images" table, but then I would need to join on those two tables when retrieving images for each product. Because SQL joins are algorithmically O(n^2), I may instead store all the images in a comma separated array occupying a single column and let my retrieving program separate the urls (comparatively inexpensive). This optimizes for reads while not really hurting write speeds.

- If there is high volume traffic to the program, well, if we were using AWS it all doesn't matter :) But if not, I suppose it depends where the bottleneck is. Too many requests to the web server can be solved by load balancing, and there are cheap and expensive ways to do this. A bit too many concurrent database connections and we can up the maximum # of connections it accepts. Way too many connections for a single db to handle, I have been told that mirroring and scaling mysql becomes somewhat of a hassle and it's better to go something like S3 or Hadoop/HBase, but I don't know too much in that department.


